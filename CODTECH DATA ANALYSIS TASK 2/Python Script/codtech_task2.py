# -*- coding: utf-8 -*-
"""CODTECH-TASK2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uLTaRcTLYYdm47XLmq5-f-mw_e1pdS0s
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score, GridSearchCV

# Set random seed for reproducibility
np.random.seed(42)

# Load the diabetes dataset
diabetes = load_diabetes()
X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
y = pd.Series(diabetes.target, name='target')

# Display basic information
print("Dataset shape:", X.shape)
print("\nFirst 5 rows:")
display(X.head())

# Display target variable
print("\nTarget variable:")
display(y.head())

# Basic statistics
print("\nFeature statistics:")
display(X.describe())

# Target variable distribution
plt.figure(figsize=(8, 5))
sns.histplot(y, kde=True)
plt.title('Distribution of Target Variable (Disease Progression)')
plt.xlabel('Target Value')
plt.ylabel('Frequency')
plt.show()

# Check for missing values
print("Missing values in features:")
print(X.isnull().sum())

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print(f"\nTraining set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")

# Feature selection using SelectKBest
selector = SelectKBest(score_func=f_regression, k='all')
selector.fit(X_train, y_train)

# Get feature scores
feature_scores = pd.DataFrame({
    'Feature': X.columns,
    'Score': selector.scores_,
    'P-value': selector.pvalues_
}).sort_values('Score', ascending=False)

print("Feature importance scores:")
display(feature_scores)

# Visualize feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Score', y='Feature', data=feature_scores)
plt.title('Feature Importance Scores')
plt.show()

# Select top 5 features
top_features = feature_scores['Feature'].head(5).values
print("\nSelected top 5 features:", top_features)

# Reduce datasets to selected features
X_train_selected = X_train[top_features]
X_test_selected = X_test[top_features]

# Initialize models
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Random Forest': RandomForestRegressor(random_state=42)
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    # Train model
    model.fit(X_train_selected, y_train)

    # Make predictions
    y_pred = model.predict(X_test_selected)

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store results
    results[name] = {
        'Model': model,
        'MSE': mse,
        'R2': r2
    }

    # Print results
    print(f"\n{name} Performance:")
    print(f"MSE: {mse:.2f}")
    print(f"R2 Score: {r2:.2f}")

# Convert results to DataFrame for comparison
results_df = pd.DataFrame(results).T
results_df = results_df[['MSE', 'R2']].sort_values('R2', ascending=False)
print("\nModel Comparison:")
display(results_df)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV
rf = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                          cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Perform grid search
grid_search.fit(X_train_selected, y_train)

# Get best model
best_rf = grid_search.best_estimator_

# Evaluate best model
y_pred_best = best_rf.predict(X_test_selected)
mse_best = mean_squared_error(y_test, y_pred_best)
r2_best = r2_score(y_test, y_pred_best)

print("\nBest Random Forest Parameters:", grid_search.best_params_)
print(f"Best MSE: {mse_best:.2f}")
print(f"Best R2 Score: {r2_best:.2f}")

# Compare with baseline
baseline_r2 = results['Random Forest']['R2']
improvement = (r2_best - baseline_r2) / baseline_r2 * 100
print(f"\nImprovement over baseline: {improvement:.1f}%")

# Get feature importance from best model
feature_importance = pd.DataFrame({
    'Feature': top_features,
    'Importance': best_rf.feature_importances_
}).sort_values('Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance)
plt.title('Feature Importance from Best Random Forest Model')
plt.show()

# Cross-validation on the entire dataset (selected features)
X_selected = X_scaled[top_features]
cv_scores = cross_val_score(best_rf, X_selected, y, cv=5, scoring='r2')

print("Cross-validation R2 scores:", cv_scores)
print(f"Mean CV R2 score: {np.mean(cv_scores):.2f} (Â±{np.std(cv_scores):.2f})")

# Plot actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_best, alpha=0.6)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values (Best Model)')
plt.show()

# Summary of results
print("Final Model Summary:")
print(f"Algorithm: Random Forest Regressor")
print(f"Selected Features: {list(top_features)}")
print(f"Test MSE: {mse_best:.2f}")
print(f"Test R2: {r2_best:.2f}")
print(f"Mean CV R2: {np.mean(cv_scores):.2f}")

# Save the best model
import joblib
joblib.dump(best_rf, 'best_diabetes_model.pkl')
print("\nBest model saved as 'best_diabetes_model.pkl'")